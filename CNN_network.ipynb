{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e293ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb28a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"zlatan599/garbage-dataset-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "path = os.path.join(path,'Garbage_Dataset_Classification' ,'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b01097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n",
    "DROP_OUT = 0.3\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_CLASSES = 6\n",
    "FINE_TUNE_LEARNING_RATE = 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2dcc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    full_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        labels='inferred',\n",
    "        label_mode='int',\n",
    "        color_mode='rgb',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=IMG_SIZE,\n",
    "        shuffle=True,  # Randomly shuffles all images\n",
    "        seed=123,      # Makes shuffle reproducible\n",
    "    )   \n",
    "\n",
    "    total_size = tf.data.experimental.cardinality(full_dataset).numpy()\n",
    "    train_size = int(0.8 * total_size)  # 80% of batches\n",
    "    val_size = int(0.1 * total_size)    # 10% of batches\n",
    "    \n",
    "    train_dataset = full_dataset.take(train_size)     # First 80% of batches\n",
    "    remaining = full_dataset.skip(train_size)         # Skip first 80%, get last 20%\n",
    "    val_dataset = remaining.take(val_size)            # First 10% of remaining (so 10% of total)\n",
    "    test_dataset = remaining.skip(val_size)           # Skip the validation part, get final 10%\n",
    "    \n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noramalize(train,val,test):\n",
    "    normalization_layer = layers.Rescaling(1./255)\n",
    "    train = train.map(lambda x, y: (normalization_layer(x), y))\n",
    "    val = val.map(lambda x, y: (normalization_layer(x), y))\n",
    "    test = test.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    train = train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "    val = val.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    test = test.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    return train,val,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae7b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig,val_orig,test_orig = load_data()\n",
    "train,val,test = noramalize(train_orig,val_orig,test_orig)\n",
    "NUM_CLASSES = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd563591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_distribution(dataset, dataset_name, num_batches=None):\n",
    "    \"\"\"Check class distribution in a tf.data.Dataset\"\"\"\n",
    "    all_labels = []\n",
    "    batch_count = 0\n",
    "    \n",
    "    # If num_batches is None, check all batches (might be slow)\n",
    "    dataset_to_check = dataset.take(num_batches) if num_batches else dataset\n",
    "    \n",
    "    for images, labels in dataset_to_check:\n",
    "        # Handle both single labels and batch of labels\n",
    "        if len(labels.shape) == 0:  # Single label (batch_size=1)\n",
    "            all_labels.append(labels.numpy())\n",
    "        else:  # Batch of labels\n",
    "            all_labels.extend(labels.numpy())\n",
    "        batch_count += 1\n",
    "    \n",
    "    # Count each class\n",
    "    unique_labels, counts = np.unique(all_labels, return_counts=True)\n",
    "    total_samples = len(all_labels)\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Distribution:\")\n",
    "    print(f\"Total samples checked: {total_samples} (from {batch_count} batches)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        percentage = (count / total_samples) * 100\n",
    "        print(f\"Class {label}: {count:4d} samples ({percentage:5.1f}%)\")\n",
    "    \n",
    "    return dict(zip(unique_labels, counts))\n",
    "\n",
    "# Usage:\n",
    "train_dist = check_class_distribution(train, \"Training\", num_batches=100)\n",
    "val_dist = check_class_distribution(val, \"Validation\")\n",
    "test_dist = check_class_distribution(test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_orig.take(5):\n",
    "    print(f\"Test batch labels: {labels.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368cfd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 13\n",
    "example = train_orig.unbatch().take(1).as_numpy_iterator().next()\n",
    "\n",
    "exaples = train_orig.unbatch().take(9).as_numpy_iterator()\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "\n",
    "for i, (image, label) in enumerate(exaples):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image.astype(\"uint8\"))\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d3077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentor():\n",
    "\n",
    "    data_aug = keras.models.Sequential()\n",
    "    data_aug.add(layers.RandomFlip(\"horizontal_and_vertical\"))\n",
    "    data_aug.add(layers.RandomRotation(0.2))\n",
    "\n",
    "    return data_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(augmentation_layer=data_augmentor()):\n",
    "    IMG_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "    preprocess_input = keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "    base_model = keras.applications.MobileNetV2(\n",
    "        input_shape=IMG_SHAPE,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # inputs = keras.Input(shape=IMG_SHAPE)\n",
    "\n",
    "    # x = augmentation_layer(inputs)\n",
    "    # x = base_model(x, training=False)\n",
    "    # x = layers.GlobalAveragePooling2D()(x)\n",
    "    # x = layers.Dropout(DROP_OUT)(x)\n",
    "    # x = layers.Dense(128, activation='relu')(x)\n",
    "    # outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    #model = keras.Model(inputs, outputs)   \n",
    "\n",
    "    \n",
    "    model = keras.models.Sequential([\n",
    "        keras.Input(shape=IMG_SHAPE),\n",
    "        augmentation_layer,\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dropout(DROP_OUT),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(DROP_OUT),\n",
    "        layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3790efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = model()\n",
    "\n",
    "model = model()  # Create your model\n",
    "print(model.summary())\n",
    "\n",
    "# Check the last layer specifically:\n",
    "print(f\"Output layer units: {model.layers[-1].units}\")\n",
    "print(f\"NUM_CLASSES setting: {NUM_CLASSES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ea58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./models/Full_garbage.weights.h5'):\n",
    "    train_model.load_weights('./models/Full_garbage.weights.h5')\n",
    "\n",
    "train_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./models/Full_garbage.weights.h5',\n",
    "    monitor='val_accuracy',  # or 'val_loss'\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,  # saves entire model\n",
    "    mode='max',  # 'max' for accuracy, 'min' for loss\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    history = train_model.fit(\n",
    "        train,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499416b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tune_model = train_model.layers[2] \n",
    "# tune_model.trainable = True\n",
    "\n",
    "# # Fine-tune from this layer onwards\n",
    "# fine_tune_at = 80\n",
    "\n",
    "# # Freeze all layers before the `fine_tune_at` layer\n",
    "# for layer in tune_model.layers[:fine_tune_at]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# tune_model.compile(\n",
    "#     optimizer=keras.optimizers.Adam(learning_rate=FINE_TUNE_LEARNING_RATE),\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# with tf.device('/GPU:0'):\n",
    "#     fine_tune_history = train_model.fit(\n",
    "#         train,\n",
    "#         epochs=EPOCHS,\n",
    "#         validation_data=val,\n",
    "#         callbacks=[checkpoint_callback]\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3122a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [0.] + history.history['accuracy']\n",
    "val_acc = [0.] + history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,2])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eabea09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - accuracy: 0.9035 - loss: 0.4820\n",
      "Test accuracy: 0.9035, Test loss: 0.4820\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "if os.path.exists('./models/best_garbage.weights.h5'):\n",
    "    train_model.load_weights('./models/best_garbage.weights.h5')\n",
    "    \n",
    "test_loss, test_accuracy = train_model.evaluate(test)\n",
    "print(f'Test accuracy: {test_accuracy:.4f}, Test loss: {test_loss:.4f}')\n",
    "# -*- coding: utf-8 -*-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
